{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d33400f-3647-4a27-80c7-870c80df680c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 10:33:51.108788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No file or directory found at nn_twice_as_big_model.keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction_label, prediction_value\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Load the new trained neural network model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn_twice_as_big_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Set the maximum sequence length (adjust as needed)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m871\u001b[39m  \u001b[38;5;66;03m# Adjust this value to match the expected input shape of your model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    264\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at nn_twice_as_big_model.keras"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to tokenize and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Convert words to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # Join words back into a sentence\n",
    "    processed_text = ' '.join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def predict_paper(model, test_input_paper, tokenizer, max_len, threshold=0.37):\n",
    "    # Preprocess the paper text\n",
    "    processed_text = preprocess_text(test_input_paper)\n",
    "\n",
    "    # Tokenize and convert text to sequences\n",
    "    sequences = tokenizer.texts_to_sequences([processed_text])\n",
    "\n",
    "    # Pad sequences to the correct length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "    # Use the trained model to predict probabilities\n",
    "    probabilities = model.predict(padded_sequences)\n",
    "\n",
    "    # Extract the predicted class and confidence\n",
    "    prediction_value = probabilities[0][0]\n",
    "\n",
    "    # Set the threshold for predicting \"ChatGPT\"\n",
    "    if prediction_value >= threshold:\n",
    "        prediction_label = \"Human generated\"\n",
    "    else:\n",
    "        prediction_label = \"ChatGPT\"\n",
    "\n",
    "    return prediction_label, prediction_value\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the new trained neural network model\n",
    "    model = tf.keras.models.load_model('nn_twice_as_big_model.keras')\n",
    "\n",
    "    # Set the maximum sequence length (adjust as needed)\n",
    "    max_len = 871  # Adjust this value to match the expected input shape of your model\n",
    "\n",
    "    # Tokenizer for text processing\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Folder containing test input files\n",
    "    folder_path = 'Test_Inputs'\n",
    "\n",
    "    # Iterate over files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Load the paper text from file\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                test_input_paper = file.read()\n",
    "\n",
    "            # Tokenize text\n",
    "            tokenizer.fit_on_texts([test_input_paper])\n",
    "\n",
    "            # Example usage\n",
    "            prediction_label, prediction_value = predict_paper(model, test_input_paper, tokenizer, max_len)\n",
    "\n",
    "            # Display the prediction for each file\n",
    "            print(f\"File: {filename}, Prediction: {prediction_label}, Confidence: {prediction_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d032d-7d84-477f-9b11-be4ece052a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf8e22-a5cd-4f78-85f0-e2eb983e9249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
